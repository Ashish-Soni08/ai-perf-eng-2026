# AI Performance Engineering 2026 — Admission Assignment

*This document contains the original assignment prompt from [Nebius Academy](https://taskchecker.academy.nebius.com/tasks/ai-performance-engineering-2026).*

---

## Overview

In this task, you will build a simple API service that takes a GitHub repository URL and returns a human-readable summary of the project: what it does, what technologies are used, and how it's structured.

This task evaluates your ability to work with external APIs, process code repositories, and use LLMs effectively.

## Technical Requirements

- **Language**: Python 3.10+
- **Web framework**: FastAPI or Flask (your choice)
- **LLM**: Nebius Token Factory API or an alternative LLM provider
- You choose which LLM model to use from the available models on Nebius Token Factory or an alternative LLM provider

## Endpoint

### `POST /summarize`

Accepts a GitHub repository URL, fetches the repository contents, and returns a summary generated by an LLM.

**Request body:**

```json
{ "github_url": "https://github.com/psf/requests" }
```

**Response:**

```json
{ 
  "summary": "**Requests** is a popular Python library for making HTTP requests...", 
  "technologies": ["Python", "urllib3", "certifi"], 
  "structure": "The project follows a standard Python package layout with the main source code in `src/requests/`, tests in `tests/`, and documentation in `docs/`." 
}
```

On error, return an appropriate HTTP status code and:

```json
{ "status": "error", "message": "Description of what went wrong" }
```

## Key Challenges

The interesting part of this task is how you handle the repository contents before sending them to the LLM:

- Repositories can be large — you can't just send everything to the LLM.
- You need to decide which files are important and which can be ignored (e.g., binary files, lock files, `node_modules/`, etc.).
- The LLM has a limited context window — you need a strategy for fitting the most relevant information.
- Think about what gives the LLM the best understanding of a project: README? Directory tree? Key source files? Config files?

There is no single correct approach — we want to see how you think about this problem.

## How Your Code Should Run

We will evaluate your solution by following the instructions in your README. Please make sure that:

1. Your README contains step-by-step instructions to install dependencies and start the server.
2. After following your instructions, the server starts and exposes the `POST /summarize` endpoint.
3. We can test it by sending a request like:

   ```bash
   curl -X POST http://localhost:8000/summarize \
     -H "Content-Type: application/json" \
     -d '{"github_url": "https://github.com/psf/requests"}'
   ```

4. LLM API key is configured via the `NEBIUS_API_KEY` environment variable (we will provide our own key when testing). Do not hardcode API keys.

That's it — if we can follow your README and get a working response, you're good.

## What to Submit

1. Working source code for the API service
2. `requirements.txt` (or equivalent) with all dependencies
3. `README.md` with:
   - Step-by-step setup and run instructions (assume a clean machine with Python installed)
   - Which model you chose and why (1–2 sentences is fine)
   - Your approach to handling repository contents (what you include, what you skip, and why)

Upload your solution as a zip archive at the Submit tab.

## Evaluation Criteria (100 points)

### Blocking criteria (0 points if not met)

These must be met for the submission to be evaluated:

- The server starts following the instructions in the README
- The `POST /summarize` endpoint exists and accepts the specified request format
- The endpoint returns a response (not an error) for a valid public GitHub repository
- The Nebius Token Factory (or an alternative provider) API is used for LLM calls
